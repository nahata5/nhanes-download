{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from time import gmtime, strftime, localtime\n",
    "import glob\n",
    "from sqlalchemy import create_engine\n",
    "# import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cdc_url = 'https://wwwn.cdc.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_links(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    table = soup.find(lambda tag: tag.has_attr('id') and tag['id']==\"GridView1\")\n",
    "    \n",
    "# Lambda expression for all links that end with XPT\n",
    "    link_list = table.findAll(lambda tag: tag.name=='a' and tag['href'].endswith(\".XPT\"))\n",
    "    links_only = [link.get('href') for link in link_list]\n",
    "    \n",
    "    return links_only\n",
    "\n",
    "# This gets all of the links for the multiple years of data listed in year_list in order to batch download files\n",
    "def get_multi_year(data_type, base_url):\n",
    "    datatype_dict = {'demographics':'Demographics', 'dietary':'Dietary',\n",
    "                     'examination':'Examination', 'laboratory':'Laboratory', \n",
    "                     'questionnaire':'Questionnaire'}\n",
    "    # Can add years as future years are added\n",
    "    year_list = [1999, 2001, 2003, 2005, 2007, 2009, 2011, 2013, 2015]\n",
    "    data_links = []\n",
    "    for year in year_list:\n",
    "        url = f\"{base_url}/nchs/nhanes/search/datapage.aspx?Component={datatype_dict[data_type]}&CycleBeginYear={year}\"\n",
    "        temp_data_links = get_table_links(url)\n",
    "        for data in temp_data_links:\n",
    "            if data not in data_links:\n",
    "                data_links.append(data)\n",
    "                print(f\"Added {data} from {year}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    return data_links\n",
    "\n",
    "# Can use the link of filename.htm on top of base_cdc_url for access to the codebook links. This will produce a dictionary of column names can replace\n",
    "def get_column_labels(xpt_link_name, base_url):\n",
    "    htm_filename = f'{xpt_link_name[:-3]}htm'\n",
    "    r = requests.get(f'{base_url}{htm_filename}')\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    # Codebook section of documentation\n",
    "    # TODO -- take section or pdf htm pages\n",
    "    codebook_links = soup.findAll('div', id='CodebookLinks')[0].findAll('a')\n",
    "    \n",
    "    dictionary = {link.string.split('-')[0].strip() : link.string.split('-')[1].strip() for link in codebook_links}\n",
    "    return dictionary\n",
    "\n",
    "# Batch download function based off of data_type ['demographics', 'examination', 'dietary', 'laboratory', 'questionnaire']\n",
    "def download_data(data_type, link_list, base_url):\n",
    "    cwd = os.getcwd()\n",
    "    try:\n",
    "        os.mkdir(data_type)\n",
    "        print(f'Created {data_type} folder')\n",
    "    except:\n",
    "        print(f'{data_type} folder exists')\n",
    "    for link in link_list:\n",
    "        item_name = link.split('/')[-1]\n",
    "        exists = os.path.isfile(f'{cwd}/{data_type}/{item_name}')\n",
    "        if exists:\n",
    "            print(f'{item_name} already exists')\n",
    "        else:\n",
    "            current_time = time.time()\n",
    "            print(f'Downloading {item_name} at {strftime(\"%a, %d %b %Y %H:%M:%S\", localtime())}')\n",
    "            r = requests.get(base_url + link, allow_redirects=True)\n",
    "            open(f'{cwd}/{data_type}/{item_name}', 'wb').write(r.content)\n",
    "            time_elapsed = time.time() - current_time\n",
    "            print(f'Downloaded {item_name} at {time_elapsed}s')\n",
    "\n",
    "# Create a dictionary of filenames for database\n",
    "def create_xpt_dict(data_type):\n",
    "    original_file_names = {}\n",
    "    group_file_names = []\n",
    "    for file in glob.glob(f'{data_type}/*'):\n",
    "        xpt_file = file.split('/')[1]\n",
    "        if len(xpt_file.split('_'))== 1:\n",
    "            original_file_names[xpt_file.split('.')[0]] = [xpt_file]\n",
    "    for file in glob.glob(f'{data_type}/*'):\n",
    "        xpt_file = file.split('/')[1]\n",
    "        if len(xpt_file.split('_'))> 1:\n",
    "            try:\n",
    "                xpt_name = xpt_file.split('_')[0]\n",
    "                original_file_names[f'{xpt_name}'].append(xpt_file)\n",
    "            except KeyError as e:\n",
    "                xpt_name = xpt_file.split('_')[0]\n",
    "                original_file_names[f'{xpt_name}'] = [xpt_file]               \n",
    "    return original_file_names\n",
    "        \n",
    "# Concat tables based on file names\n",
    "def combine_tables(data_type, xpt_dict):\n",
    "    temp_df_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for keys, values in xpt_dict[data_type].items():\n",
    "        for value in values:\n",
    "            print(f'Trying {cwd}/{data_type}/{value}')\n",
    "            temp_df_list.append(pd.read_sas(f'{cwd}/{data_type}/{value}'))\n",
    "            print(f'{cwd}/{data_type}/{value} appended')\n",
    "    return pd.concat(temp_df_list)\n",
    "                  \n",
    "# Error handling if downloading empty files\n",
    "def grab_empty_files(data_type, base_url):\n",
    "    empty_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for file in glob.glob(f'{cwd}/{data_type}/*'):\n",
    "        if os.stat(file).st_size == 0:\n",
    "            empty_list.append(file)\n",
    "            os.remove(file)\n",
    "    if len(empty_list) == 0:\n",
    "        print(\"There are no empty files in this folder\")\n",
    "    else:\n",
    "        print(f\"Now re-downloading {len(empty_list)} files\")\n",
    "        download_data(data_type, empty_list, base_url)\n",
    "\n",
    "# demographic_links = get_multi_year('demographics', base_cdc_url)\n",
    "# dietary_links = get_multi_year('dietary', base_cdc_url)\n",
    "# examination_links = get_multi_year('examination', base_cdc_url)\n",
    "# laboratory_links = get_multi_year('laboratory', base_cdc_url)\n",
    "# questionnaire_links = get_multi_year('questionnaire', base_cdc_url)\n",
    "\n",
    "# link_dictionary = {'demographics':demographic_links, 'dietary':dietary_links, \n",
    "#                    'examination':examination_links, 'laboratory':laboratory_links,\n",
    "#                   'questionnaire':questionnaire_links}\n",
    "# with open('xpt_link_dict.json', 'w') as f:\n",
    "#     json.dump(link_dictionary, f)\n",
    "    \n",
    "# Create the xpt_file_dict json for individual table creation and anticipation of merged tabes\n",
    "# xpt_file_dict = {}\n",
    "# for keys in link_dictionary:\n",
    "#     xpt_file_dict[keys] = create_xpt_dict(keys)\n",
    "    \n",
    "# with open('xpt_file_dict.json', 'w') as f:\n",
    "#     json.dump(xpt_file_dict, f)\n",
    "    \n",
    "\n",
    "    \n",
    "# # Download data - \n",
    "# download_data('demographics', xpt_link_dictionary['demographics'], base_cdc_url)\n",
    "# download_data('dietary', xpt_link_dictionary['dietary'], base_cdc_url)\n",
    "# download_data('examination', xpt_link_dictionary['examination'], base_cdc_url)\n",
    "# download_data('laboratory', xpt_link_dictionary['laboratory'], base_cdc_url)\n",
    "# download_data('questionnaire', xpt_link_dictionary['questionnaire'], base_cdc_url)\n",
    "\n",
    "\n",
    "#Ensure DB max_allowed_packet is set to 1G, this funciton will send to a mysql database\n",
    "def send_to_db(user,password,host,port,database, data_type, link_dict, file_dict):\n",
    "    engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}:{port}/{database}', \n",
    "                           echo=False)\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    counter = 0\n",
    "    db_name_dict = create_db_names(data_type, link_dict, file_dict)\n",
    "    for file in glob.glob(f'{cwd}/{data_type}/*'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(f\"Creating dataframe from {file}\")\n",
    "        temp_df = pd.read_sas(file, encoding='ISO-8859-1')\n",
    "        print(f'Sending to MySQL Server as {db_name_dict[file_name][1]}')\n",
    "        try:\n",
    "            temp_df.to_sql(name=f'{db_name_dict[file_name][1]}', con=engine, if_exists='fail', index=False)\n",
    "            counter += 1\n",
    "        except ValueError as e:\n",
    "            print(file_name + \"is present\")\n",
    "            print(e)\n",
    "            \n",
    "        print('Now cleaning up db')\n",
    "        del temp_df\n",
    "    print(f'Added {counter} databases')\n",
    "\n",
    "\n",
    "\n",
    "#   This will create file names that append the start year last 2 digits ie. 99 for 1999 and prefix DIET, DEMO, LAB, EXAM, QUEST for the respective filename. It will use the base file name ie. DEMO from DEMO_H.XPT as the filename\n",
    "def create_db_names(data_type, link_dict, file_dict):\n",
    "    \n",
    "#   Exludes a DEMO preview because single tables do not need DEMO_DEMO\n",
    "    prefix_dict = {'demographics': '', 'dietary': 'DIET_', 'examination': 'EXAM_', \n",
    "                   'laboratory': 'LAB_', 'questionnaire': 'QUEST_'}\n",
    "    \n",
    "    temp_dict = {}\n",
    "    \n",
    "#   Create temp_dict[filename:['2digit year']]\n",
    "    for link in xpt_link_dictionary[data_type]:\n",
    "            temp_dict[link.split('/')[-1]] = [link.split('/')[-2][2:4]]\n",
    "            \n",
    "#   Add prefix and DB name to temp_dict[xpt_filename: ['2digit year', 'DB Name example DIET_DSBI_99']]\n",
    "    for key, values in xpt_file_dictionary[data_type].items():\n",
    "        for value in values:\n",
    "            if len(value.split('_')) > 2:\n",
    "                #If there are multiple for same  year in sequence for instance lipids second value\n",
    "                temp_dict[value].append(f'{prefix_dict[data_type]}'+ value[:-6] + \"_\" + temp_dict[value][0])\n",
    "            else:\n",
    "                temp_dict[value].append(f'{prefix_dict[data_type]}'+ key + \"_\" + temp_dict[value][0])\n",
    "    \n",
    "    return temp_dict\n",
    "\n",
    "#Setting UTF8 and latin1 encoding errors \n",
    "#DSII does not play nice with UTF and encoding errors row '\\xC2\\x92S MU...' for column 'DSDSUPP' at row 74590\n",
    "#DSPI Incorrect string value: '\\xC2\\x92S MU...' for column 'DSDSUPP' at row 6913\n",
    "\n",
    "# Send folder of files and links to feather dataframes\n",
    "def send_to_feather(data_type, link_dict, file_dict):    \n",
    "    cwd = os.getcwd()\n",
    "    counter = 0\n",
    "    feather_name_dict = create_db_names(data_type, link_dict, file_dict)\n",
    "    for file in glob.glob(f'{cwd}/{data_type}/*'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(f\"Creating dataframe from {file}\")\n",
    "        temp_df = pd.read_sas(file)\n",
    "        print(f'Sending to Feather as {feather_name_dict[file_name][1]}')\n",
    "        try:\n",
    "            temp_df.to_feather(f'{cwd}/{data_type}_feather/{feather_name_dict[file_name][1]}.feather')\n",
    "            counter += 1\n",
    "        except ValueError as e:\n",
    "            print(file_name + \"is present\")\n",
    "            print(e)    \n",
    "        print('Now cleaning up dataframe')\n",
    "        del temp_df\n",
    "    print(f'Added {counter} feather dataframes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+mysqlconnector://tom:password@localhost:3306/nhanes', echo=False)\n",
    "send_to_db('tom', 'password', 'localhost','3306', 'nhanes', 'dietary', xpt_link_dictionary, xpt_file_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpt_link_dictionary = json.loads(open('xpt_link_dict.json').read())\n",
    "xpt_file_dictionary = json.loads(open('xpt_file_dict.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df99 = pd.read_sas(\"demographics/DEMO.XPT\")\n",
    "df01 = pd.read_sas('demographics/DEMO_B.XPT')\n",
    "df03 = pd.read_sas('demographics/DEMO_C.XPT')\n",
    "df05 = pd.read_sas('demographics/DEMO_D.XPT')\n",
    "df07 = pd.read_sas('demographics/DEMO_E.XPT')\n",
    "df09 = pd.read_sas('demographics/DEMO_F.XPT')\n",
    "df11 = pd.read_sas('demographics/DEMO_G.XPT')\n",
    "df13 = pd.read_sas('demographics/DEMO_H.XPT')\n",
    "df15 = pd.read_sas('demographics/DEMO_I.XPT')\n",
    "\n",
    "# Concat all demographic tables without sorting columns\n",
    "df99to15 = pd.concat([df99, df01, df03,df05,df07,df09,df11,df13,df15], sort=False)\n",
    "# Removed WTIRE and WTMREP 52 columns x2, left with 66 columns\n",
    "df99to15_cleaned = df99to15[df99to15.columns[~df99to15.columns.str.match('(WTIRE|WTMREP)')]]\n",
    "df99to15_clean = df99to15_cleaned.set_index('SEQN')\n",
    "df99to15_clean.to_csv('demographics_1999-2016.csv')\n",
    "\n",
    "df99to15_clean.reset_index().to_feather('demographics_feather/demographics_1999-2016.feather')\n",
    "send_to_feather('questionnaire', xpt_link_dictionary, xpt_file_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lipid profile sas readings\n",
    "\n",
    "hdl_07 = pd.read_sas('laboratory/HDL_E.XPT')\n",
    "hdl_05 = pd.read_sas('laboratory/HDL_D.XPT')\n",
    "hdl_09 = pd.read_sas('laboratory/HDL_F.XPT')\n",
    "hdl_11 = pd.read_sas('laboratory/HDL_G.XPT')\n",
    "hdl_13 = pd.read_sas('laboratory/HDL_H.XPT')\n",
    "hdl_15 = pd.read_sas('laboratory/HDL_I.XPT')\n",
    "ldl_07 = pd.read_sas('laboratory/TRIGLY_E.XPT')\n",
    "ldl_99 = pd.read_sas('laboratory/LAB13AM.XPT')\n",
    "ldl_01 = pd.read_sas('laboratory/L13AM_B.XPT')\n",
    "ldl_03 = pd.read_sas('laboratory/L13AM_C.XPT')\n",
    "ldl_09 = pd.read_sas('laboratory/TRIGLY_F.XPT')\n",
    "ldl_11 = pd.read_sas('laboratory/TRIGLY_G.XPT')\n",
    "ldl_13 = pd.read_sas('laboratory/TRIGLY_H.XPT')\n",
    "ldl_05 = pd.read_sas('laboratory/TRIGLY_D.XPT')\n",
    "total_07 = pd.read_sas('laboratory/TCHOL_E.XPT')\n",
    "total_05 = pd.read_sas('laboratory/TCHOL_D.XPT')\n",
    "total_09 = pd.read_sas('laboratory/TCHOL_F.XPT')\n",
    "total_11 = pd.read_sas('laboratory/TCHOL_G.XPT')\n",
    "total_13 = pd.read_sas('laboratory/TCHOL_H.XPT')\n",
    "total_15 = pd.read_sas('laboratory/TCHOL_I.XPT')\n",
    "total_99 = pd.read_sas(\"laboratory/LAB13.XPT\")\n",
    "total_01 = pd.read_sas(\"laboratory/L13_B.XPT\")\n",
    "total_03 = pd.read_sas(\"laboratory/L13_C.XPT\")\n",
    "apo_07 = pd.read_sas('laboratory/APOB_E.XPT')\n",
    "apo_09 = pd.read_sas('laboratory/APOB_F.XPT')\n",
    "apo_11 = pd.read_sas('laboratory/APOB_G.XPT')\n",
    "apo_13 = pd.read_sas('laboratory/APOB_H.XPT')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- The issues that need to be resolved, the total chol datasets from 99-05 have total and HDL cholesterol in both, but the rest are separated out. This needs to be mitigated by separating the total and hdl into two separate dfs and then concatting all years, and then merging the two in order to get a complete set.\n",
    "\n",
    "- For whatever reason, there is no LDL file on CDC site for 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some years with various columns names, this is to standardize columns names\n",
    "hdl_columns = {'LBDHDD':\"LBDHDL\", \"LBDHDDSI\":\"LBDHDLSI\"}\n",
    "renamed_hdl_df = pd.concat([hdl_05, hdl_07, hdl_09, hdl_11, hdl_13, hdl_15], sort=False).rename(columns=hdl_columns)\n",
    "total_03 = total_03.rename(columns={\"LBXHDD\":'LBDHDL', \"LBDHDDSI\":'LBDHDLSI'})\n",
    "total_df = pd.concat([total_05, total_07, total_09, total_11, total_13, total_15], sort=False)\n",
    "\n",
    "# First we need to separate the columns from the combined datasets into HDL specific and total chol specific dataframes, also separate the LDL from apo in 2005 dataframe\n",
    "hdl99to03 = pd.concat([total_99, total_01, total_03], sort=False).loc[:, ['SEQN', 'LBDHDL', 'LBDHDLSI']]\n",
    "total99to03 = pd.concat([total_99, total_01, total_03], sort=False).loc[:, ['SEQN', 'LBXTC', 'LBDTCSI']]\n",
    "ldl_only_05 = ldl_05.iloc[:,:-2]\n",
    "apo_05 = ldl_05.loc[:,['SEQN','WTSAF2YR','LBXAPB','LBDAPBSI']]\n",
    "\n",
    "#Then we combine all of the years to create separated datasets in preparation of merging\n",
    "hdl_complete_df = pd.concat([hdl99to03, renamed_hdl_df], sort=False)\n",
    "total_complete_df = pd.concat([total_df, total99to03], sort=False)\n",
    "ldl_complete_df = pd.concat([ldl_99, ldl_01, ldl_03, ldl_only_05, ldl_07, ldl_09, ldl_11, ldl_13], sort=False)\n",
    "apo_complete_df = pd.concat([apo_05, apo_07,apo_09, apo_11, apo_13], sort=False)\n",
    "\n",
    "\n",
    "lipid_complete_df = (hdl_complete_df\n",
    "                     .merge(total_complete_df, on='SEQN', how='outer')\n",
    "                     .merge(ldl_complete_df, on='SEQN', how='outer')\n",
    "                     .merge(apo_complete_df, on=['SEQN','WTSAF2YR'], how='outer').sort_values('SEQN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>LBDHDL</th>\n",
       "      <th>LBDHDLSI</th>\n",
       "      <th>LBXTC</th>\n",
       "      <th>LBDTCSI</th>\n",
       "      <th>WTSAF2YR</th>\n",
       "      <th>WTSAF4YR</th>\n",
       "      <th>LBXTR</th>\n",
       "      <th>LBDTRSI</th>\n",
       "      <th>LBDLDL</th>\n",
       "      <th>LBDLDLSI</th>\n",
       "      <th>LBXAPB</th>\n",
       "      <th>LBDAPBSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>215.0</td>\n",
       "      <td>5.56</td>\n",
       "      <td>60586.147294</td>\n",
       "      <td>33073.267573</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>136.0</td>\n",
       "      <td>3.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>129.0</td>\n",
       "      <td>3.34</td>\n",
       "      <td>121969.841152</td>\n",
       "      <td>52434.225472</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>279.0</td>\n",
       "      <td>7.21</td>\n",
       "      <td>234895.205650</td>\n",
       "      <td>98468.806492</td>\n",
       "      <td>347.0</td>\n",
       "      <td>3.92</td>\n",
       "      <td>168.0</td>\n",
       "      <td>4.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.57</td>\n",
       "      <td>153.0</td>\n",
       "      <td>3.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>245.0</td>\n",
       "      <td>6.34</td>\n",
       "      <td>57661.621988</td>\n",
       "      <td>32935.874064</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>127.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN  LBDHDL  LBDHDLSI  LBXTC  LBDTCSI       WTSAF2YR      WTSAF4YR  LBXTR  \\\n",
       "0   2.0    54.0      1.39  215.0     5.56   60586.147294  33073.267573  128.0   \n",
       "1   3.0    30.0      0.78  129.0     3.34  121969.841152  52434.225472  202.0   \n",
       "2   5.0    42.0      1.08  279.0     7.21  234895.205650  98468.806492  347.0   \n",
       "3   6.0    61.0      1.57  153.0     3.96            NaN           NaN    NaN   \n",
       "4   7.0   105.0      2.73  245.0     6.34   57661.621988  32935.874064   62.0   \n",
       "\n",
       "   LBDTRSI  LBDLDL  LBDLDLSI  LBXAPB  LBDAPBSI  \n",
       "0     1.45   136.0      3.52     NaN       NaN  \n",
       "1     2.28    58.0      1.50     NaN       NaN  \n",
       "2     3.92   168.0      4.34     NaN       NaN  \n",
       "3      NaN     NaN       NaN     NaN       NaN  \n",
       "4     0.70   127.0      3.28     NaN       NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipid_complete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CBC for all years 99 to 15\n",
    "cbc_99 = pd.read_sas('laboratory/LAB25.XPT')\n",
    "cbc_01 = pd.read_sas('laboratory/L25_B.XPT')\n",
    "cbc_03 = pd.read_sas('laboratory/L25_C.XPT')\n",
    "cbc_05 = pd.read_sas('laboratory/CBC_D.XPT')\n",
    "cbc_07 = pd.read_sas('laboratory/CBC_E.XPT')\n",
    "cbc_09 = pd.read_sas('laboratory/CBC_F.XPT')\n",
    "cbc_11 = pd.read_sas('laboratory/CBC_G.XPT')\n",
    "cbc_13 = pd.read_sas('laboratory/CBC_H.XPT')\n",
    "cbc_15 = pd.read_sas('laboratory/CBC_I.XPT')\n",
    "\n",
    "# Need to rename and remove two columns in year 15, LBXMCHSI is in pg not in g/dl which is what it used to be in LBXMC. LBXMCH is now a new column that only has NAN data\n",
    "cbc_15 = cbc_15.rename(columns={'LBXMCHSI':'LBXMC'})\n",
    "cbc_15 = cbc_15.drop(columns='LBXMCH')\n",
    "\n",
    "#Create complete cbc df\n",
    "cbc_complete_df = pd.concat([cbc_99, cbc_01, cbc_03, cbc_05, \n",
    "                             cbc_07, cbc_09, cbc_11, cbc_13, cbc_15], sort=False).sort_values('SEQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_cr_99 = pd.read_sas('laboratory/LAB16.XPT')\n",
    "alb_cr_01 = pd.read_sas('laboratory/L16_B.XPT')\n",
    "alb_cr_03 = pd.read_sas('laboratory/L16_C.XPT')\n",
    "alb_cr_05 = pd.read_sas('laboratory/ALB_CR_D.XPT')\n",
    "alb_cr_07 = pd.read_sas('laboratory/ALB_CR_E.XPT')\n",
    "alb_cr_09 = pd.read_sas('laboratory/ALB_CR_F.XPT')\n",
    "alb_cr_11 = pd.read_sas('laboratory/ALB_CR_G.XPT')\n",
    "alb_cr_13 = pd.read_sas('laboratory/ALB_CR_H.XPT')\n",
    "alb_cr_15 = pd.read_sas('laboratory/ALB_CR_I.XPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to transform dataframes up to 07 to add calculated urine albumin/cr ratio, it was pre-calculated after 09\n",
    "# Need to multiply by  100 to change units from ug to mg URDACT = URXUMA/URXUCR x 100\n",
    "# 2009 was the only year with a second collection 10 days after initial collection\n",
    "# Therefore, the 2nd collection as like other dataframes was removed to remain consistent\n",
    "alb_cr_99.loc[:, 'URDACT'] = alb_cr_99.loc[:,'URXUMA'] * 100/ alb_cr_99.loc[:, 'URXUCR']\n",
    "alb_cr_01.loc[:, 'URDACT'] = alb_cr_01.loc[:,'URXUMA'] * 100/ alb_cr_01.loc[:, 'URXUCR']\n",
    "alb_cr_03.loc[:, 'URDACT'] = alb_cr_03.loc[:,'URXUMA'] * 100/ alb_cr_03.loc[:, 'URXUCR']\n",
    "alb_cr_05.loc[:, 'URDACT'] = alb_cr_05.loc[:,'URXUMA'] * 100/ alb_cr_05.loc[:, 'URXUCR']\n",
    "alb_cr_07.loc[:, 'URDACT'] = alb_cr_07.loc[:,'URXUMA'] * 100/ alb_cr_07.loc[:, 'URXUCR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2009 there needs to remove 2nd collection columns, please refer to original file if seeking\n",
    "alb_cr_09_trim = alb_cr_09.iloc[:, :-5]\n",
    "# Beginning in 2005, new naming convention for the SI, standardize names of prior columns as below\n",
    "alb_rename = {\"URXUMASI\":'URXUMS', 'URXUCRSI':'URXCRS'}\n",
    "alb_cr_99 = alb_cr_99.rename(columns=alb_rename)\n",
    "alb_cr_01 = alb_cr_01.rename(columns=alb_rename)\n",
    "alb_cr_03 = alb_cr_01.rename(columns=alb_rename)\n",
    "\n",
    "# Removing the \"lower limit of detection columns\" in 2015 data, doesn't exist in any other set\n",
    "alb_cr_15_trim = alb_cr_15.loc[:,['SEQN', 'URXUMA', 'URXUMS', 'URXUCR','URXCRS', 'URDACT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_dfs = [alb_cr_99, alb_cr_01,alb_cr_03, alb_cr_05,\n",
    "           alb_cr_07,alb_cr_09_trim,alb_cr_11,alb_cr_13,alb_cr_15_trim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_cr_complete_df = pd.concat(alb_dfs, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biochemistry metobolic panel\n",
    "cmp_99 = pd.read_sas('laboratory/LAB18.XPT')\n",
    "cmp_01 = pd.read_sas('laboratory/L40_B.XPT')\n",
    "cmp_03 = pd.read_sas('laboratory/L40_C.XPT')\n",
    "cmp_05 = pd.read_sas('laboratory/BIOPRO_D.XPT')\n",
    "cmp_07 = pd.read_sas('laboratory/BIOPRO_E.XPT')\n",
    "cmp_09 = pd.read_sas('laboratory/BIOPRO_F.XPT')\n",
    "cmp_11 = pd.read_sas('laboratory/BIOPRO_G.XPT')\n",
    "cmp_13 = pd.read_sas('laboratory/BIOPRO_H.XPT')\n",
    "cmp_15 = pd.read_sas('laboratory/BIOPRO_I.XPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-d88a14b6a0b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Need to remove hormone columns, should be in separate DF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#For all of biochemistry panel, included is triglycerides, we have separate DF for lipids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcmp_99\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmp_99\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# Need to remove hormone columns, should be in separate DF\n",
    "#For all of biochemistry panel, included is triglycerides, we have separate DF for lipids\n",
    "cmp_99 = cmp_99.iloc[:, :-4]\n",
    "\n",
    "# --TODO -- \n",
    "# Determine if triglycerides should be included as separate, different column names\n",
    "# Correct serum creatine for 99-00  Standard Creatinine (Y) = 1.013*NHANES Creatinine (X) + 0.147 (r = 0.984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>LBXTR</th>\n",
       "      <th>LBDTRSI</th>\n",
       "      <th>LBXSTR</th>\n",
       "      <th>LBXSTRSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>3.92</td>\n",
       "      <td>341.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>72.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>322.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.32</td>\n",
       "      <td>110.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.35</td>\n",
       "      <td>107.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1.93</td>\n",
       "      <td>162.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>184.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>170.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>34.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>232.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>42.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>44.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>46.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6728</th>\n",
       "      <td>9921.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6729</th>\n",
       "      <td>9922.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6730</th>\n",
       "      <td>9925.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>166.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6731</th>\n",
       "      <td>9926.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6732</th>\n",
       "      <td>9930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>9931.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6735</th>\n",
       "      <td>9935.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>9936.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>9937.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>207.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>9938.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>9939.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1.56</td>\n",
       "      <td>138.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6740</th>\n",
       "      <td>9940.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>1.77</td>\n",
       "      <td>158.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6741</th>\n",
       "      <td>9941.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6742</th>\n",
       "      <td>9942.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>197.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6743</th>\n",
       "      <td>9944.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>10.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6744</th>\n",
       "      <td>9946.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6745</th>\n",
       "      <td>9947.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>356.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6746</th>\n",
       "      <td>9949.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6747</th>\n",
       "      <td>9950.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6748</th>\n",
       "      <td>9951.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>9952.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6750</th>\n",
       "      <td>9953.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6751</th>\n",
       "      <td>9954.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6752</th>\n",
       "      <td>9955.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6753</th>\n",
       "      <td>9956.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6754</th>\n",
       "      <td>9958.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6755</th>\n",
       "      <td>9960.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6756</th>\n",
       "      <td>9961.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>272.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6757</th>\n",
       "      <td>9965.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>356.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6758 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SEQN  LBXTR  LBDTRSI  LBXSTR  LBXSTRSI\n",
       "0        2.0  128.0     1.45   115.0       NaN\n",
       "1        5.0  347.0     3.92   341.0       NaN\n",
       "2        6.0    NaN      NaN    49.0       NaN\n",
       "3        7.0   62.0     0.70    57.0       NaN\n",
       "4        8.0   33.0     0.37    29.0       NaN\n",
       "5       10.0   45.0     0.51    42.0       NaN\n",
       "6       11.0   76.0     0.86    72.0       NaN\n",
       "7       12.0  146.0     1.65   140.0       NaN\n",
       "8       13.0    NaN      NaN   322.0       NaN\n",
       "9       14.0    NaN      NaN    67.0       NaN\n",
       "10      15.0   54.0     0.61    67.0       NaN\n",
       "11      16.0    NaN      NaN    42.0       NaN\n",
       "12      20.0  117.0     1.32   110.0       NaN\n",
       "13      21.0  120.0     1.35   107.0       NaN\n",
       "14      22.0    NaN      NaN    61.0       NaN\n",
       "15      23.0  171.0     1.93   162.0       NaN\n",
       "16      24.0    NaN      NaN    75.0       NaN\n",
       "17      25.0  202.0     2.28   184.0       NaN\n",
       "18      26.0    NaN      NaN   115.0       NaN\n",
       "19      27.0    NaN      NaN     NaN       NaN\n",
       "20      28.0   52.0     0.59    43.0       NaN\n",
       "21      29.0  167.0     1.89   170.0       NaN\n",
       "22      31.0  128.0     1.45   118.0       NaN\n",
       "23      33.0    NaN      NaN    94.0       NaN\n",
       "24      34.0   97.0     1.10    97.0       NaN\n",
       "25      40.0    NaN      NaN   232.0       NaN\n",
       "26      42.0   62.0     0.70    53.0       NaN\n",
       "27      44.0   47.0     0.53    44.0       NaN\n",
       "28      45.0    NaN      NaN    84.0       NaN\n",
       "29      46.0  122.0     1.38   120.0       NaN\n",
       "...      ...    ...      ...     ...       ...\n",
       "6728  9921.0  147.0     1.66   130.0       NaN\n",
       "6729  9922.0    NaN      NaN    80.0       NaN\n",
       "6730  9925.0  186.0     2.10   166.0       NaN\n",
       "6731  9926.0   99.0     1.12    99.0       NaN\n",
       "6732  9930.0    NaN      NaN   112.0       NaN\n",
       "6733  9931.0    NaN      NaN   168.0       NaN\n",
       "6734  9932.0   84.0     0.95    78.0       NaN\n",
       "6735  9935.0    NaN      NaN   133.0       NaN\n",
       "6736  9936.0   47.0     0.53    47.0       NaN\n",
       "6737  9937.0    NaN      NaN   207.0       NaN\n",
       "6738  9938.0    NaN      NaN   211.0       NaN\n",
       "6739  9939.0  138.0     1.56   138.0       NaN\n",
       "6740  9940.0  157.0     1.77   158.0       NaN\n",
       "6741  9941.0    NaN      NaN    98.0       NaN\n",
       "6742  9942.0  235.0     2.65   197.0       NaN\n",
       "6743  9944.0  973.0    10.99   916.0       NaN\n",
       "6744  9946.0    NaN      NaN    35.0       NaN\n",
       "6745  9947.0    NaN      NaN   356.0       NaN\n",
       "6746  9949.0    NaN      NaN    99.0       NaN\n",
       "6747  9950.0   80.0     0.90    66.0       NaN\n",
       "6748  9951.0   66.0     0.75    55.0       NaN\n",
       "6749  9952.0   67.0     0.76    61.0       NaN\n",
       "6750  9953.0    NaN      NaN    88.0       NaN\n",
       "6751  9954.0    NaN      NaN    48.0       NaN\n",
       "6752  9955.0    NaN      NaN    86.0       NaN\n",
       "6753  9956.0    NaN      NaN   181.0       NaN\n",
       "6754  9958.0    NaN      NaN    68.0       NaN\n",
       "6755  9960.0  111.0     1.25   101.0       NaN\n",
       "6756  9961.0  277.0     3.13   272.0       NaN\n",
       "6757  9965.0    NaN      NaN   356.0       NaN\n",
       "\n",
       "[6758 rows x 5 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipid_complete_df.merge(cmp_99, on='SEQN').loc[:,['SEQN','LBXTR', 'LBDTRSI','LBXSTR','LBXSTRSI']]\n",
    "\n",
    "# comparison of these two from the lipid s, shows similar but different values for TGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
