{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from time import gmtime, strftime, localtime\n",
    "import glob\n",
    "from sqlalchemy import create_engine\n",
    "# import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cdc_url = 'https://wwwn.cdc.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_links(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    table = soup.find(lambda tag: tag.has_attr('id') and tag['id']==\"GridView1\")\n",
    "    \n",
    "# Lambda expression for all links that end with XPT\n",
    "    link_list = table.findAll(lambda tag: tag.name=='a' and tag['href'].endswith(\".XPT\"))\n",
    "    links_only = [link.get('href') for link in link_list]\n",
    "    \n",
    "    return links_only\n",
    "\n",
    "# This gets all of the links for the multiple years of data listed in year_list in order to batch download files\n",
    "def get_multi_year(data_type, base_url):\n",
    "    datatype_dict = {'demographics':'Demographics', 'dietary':'Dietary',\n",
    "                     'examination':'Examination', 'laboratory':'Laboratory', \n",
    "                     'questionnaire':'Questionnaire'}\n",
    "    # Can add years as future years are added\n",
    "    year_list = [1999, 2001, 2003, 2005, 2007, 2009, 2011, 2013, 2015]\n",
    "    data_links = []\n",
    "    for year in year_list:\n",
    "        url = f\"{base_url}/nchs/nhanes/search/datapage.aspx?Component={datatype_dict[data_type]}&CycleBeginYear={year}\"\n",
    "        temp_data_links = get_table_links(url)\n",
    "        for data in temp_data_links:\n",
    "            if data not in data_links:\n",
    "                data_links.append(data)\n",
    "                print(f\"Added {data} from {year}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    return data_links\n",
    "\n",
    "# Can use the link of filename.htm on top of base_cdc_url for access to the codebook links. This will produce a dictionary of column names can replace\n",
    "def get_column_labels(xpt_link_name, base_url):\n",
    "    htm_filename = f'{xpt_link_name[:-3]}htm'\n",
    "    r = requests.get(f'{base_url}{htm_filename}')\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    # Codebook section of documentation\n",
    "    # TODO -- take section or pdf htm pages\n",
    "    codebook_links = soup.findAll('div', id='CodebookLinks')[0].findAll('a')\n",
    "    \n",
    "    dictionary = {link.string.split('-')[0].strip() : link.string.split('-')[1].strip() for link in codebook_links}\n",
    "    return dictionary\n",
    "\n",
    "# Batch download function based off of data_type ['demographics', 'examination', 'dietary', 'laboratory', 'questionnaire']\n",
    "def download_data(data_type, link_list, base_url):\n",
    "    cwd = os.getcwd()\n",
    "    try:\n",
    "        os.mkdir(data_type)\n",
    "        print(f'Created {data_type} folder')\n",
    "    except:\n",
    "        print(f'{data_type} folder exists')\n",
    "    for link in link_list:\n",
    "        item_name = link.split('/')[-1]\n",
    "        exists = os.path.isfile(f'{cwd}/{data_type}/{item_name}')\n",
    "        if exists:\n",
    "            print(f'{item_name} already exists')\n",
    "        else:\n",
    "            current_time = time.time()\n",
    "            print(f'Downloading {item_name} at {strftime(\"%a, %d %b %Y %H:%M:%S\", localtime())}')\n",
    "            r = requests.get(base_url + link, allow_redirects=True)\n",
    "            open(f'{cwd}/{data_type}/{item_name}', 'wb').write(r.content)\n",
    "            time_elapsed = time.time() - current_time\n",
    "            print(f'Downloaded {item_name} at {time_elapsed}s')\n",
    "\n",
    "# Create a dictionary of filenames for database\n",
    "def create_xpt_dict(data_type):\n",
    "    original_file_names = {}\n",
    "    group_file_names = []\n",
    "    for file in glob.glob(f'{data_type}/*'):\n",
    "        xpt_file = file.split('/')[1]\n",
    "        if len(xpt_file.split('_'))== 1:\n",
    "            original_file_names[xpt_file.split('.')[0]] = [xpt_file]\n",
    "    for file in glob.glob(f'{data_type}/*'):\n",
    "        xpt_file = file.split('/')[1]\n",
    "        if len(xpt_file.split('_'))> 1:\n",
    "            try:\n",
    "                xpt_name = xpt_file.split('_')[0]\n",
    "                original_file_names[f'{xpt_name}'].append(xpt_file)\n",
    "            except KeyError as e:\n",
    "                xpt_name = xpt_file.split('_')[0]\n",
    "                original_file_names[f'{xpt_name}'] = [xpt_file]               \n",
    "    return original_file_names\n",
    "        \n",
    "# Concat tables based on file names\n",
    "def combine_tables(data_type, xpt_dict):\n",
    "    temp_df_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for keys, values in xpt_dict[data_type].items():\n",
    "        for value in values:\n",
    "            print(f'Trying {cwd}/{data_type}/{value}')\n",
    "            temp_df_list.append(pd.read_sas(f'{cwd}/{data_type}/{value}'))\n",
    "            print(f'{cwd}/{data_type}/{value} appended')\n",
    "    return pd.concat(temp_df_list)\n",
    "                  \n",
    "# Error handling if downloading empty files\n",
    "def grab_empty_files(data_type, base_url):\n",
    "    empty_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for file in glob.glob(f'{cwd}/{data_type}/*'):\n",
    "        if os.stat(file).st_size == 0:\n",
    "            empty_list.append(file)\n",
    "            os.remove(file)\n",
    "    if len(empty_list) == 0:\n",
    "        print(\"There are no empty files in this folder\")\n",
    "    else:\n",
    "        print(f\"Now re-downloading {len(empty_list)} files\")\n",
    "        download_data(data_type, empty_list, base_url)\n",
    "\n",
    "# demographic_links = get_multi_year('demographics', base_cdc_url)\n",
    "# dietary_links = get_multi_year('dietary', base_cdc_url)\n",
    "# examination_links = get_multi_year('examination', base_cdc_url)\n",
    "# laboratory_links = get_multi_year('laboratory', base_cdc_url)\n",
    "# questionnaire_links = get_multi_year('questionnaire', base_cdc_url)\n",
    "\n",
    "# link_dictionary = {'demographics':demographic_links, 'dietary':dietary_links, \n",
    "#                    'examination':examination_links, 'laboratory':laboratory_links,\n",
    "#                   'questionnaire':questionnaire_links}\n",
    "# with open('xpt_link_dict.json', 'w') as f:\n",
    "#     json.dump(link_dictionary, f)\n",
    "    \n",
    "# Create the xpt_file_dict json for individual table creation and anticipation of merged tabes\n",
    "# xpt_file_dict = {}\n",
    "# for keys in link_dictionary:\n",
    "#     xpt_file_dict[keys] = create_xpt_dict(keys)\n",
    "    \n",
    "# with open('xpt_file_dict.json', 'w') as f:\n",
    "#     json.dump(xpt_file_dict, f)\n",
    "    \n",
    "\n",
    "    \n",
    "# # Download data - \n",
    "# download_data('demographics', xpt_link_dictionary['demographics'], base_cdc_url)\n",
    "# download_data('dietary', xpt_link_dictionary['dietary'], base_cdc_url)\n",
    "# download_data('examination', xpt_link_dictionary['examination'], base_cdc_url)\n",
    "# download_data('laboratory', xpt_link_dictionary['laboratory'], base_cdc_url)\n",
    "# download_data('questionnaire', xpt_link_dictionary['questionnaire'], base_cdc_url)\n",
    "\n",
    "\n",
    "#Ensure DB max_allowed_packet is set to 1G, this funciton will send to a mysql database\n",
    "def send_to_db(user,password,host,port,database, data_type, link_dict, file_dict):\n",
    "    engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}:{port}/{database}', \n",
    "                           echo=False)\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    counter = 0\n",
    "    db_name_dict = create_db_names(data_type, link_dict, file_dict)\n",
    "    for file in glob.glob(f'{cwd}/{data_type}/*'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(f\"Creating dataframe from {file}\")\n",
    "        temp_df = pd.read_sas(file, encoding='ISO-8859-1')\n",
    "        print(f'Sending to MySQL Server as {db_name_dict[file_name][1]}')\n",
    "        try:\n",
    "            temp_df.to_sql(name=f'{db_name_dict[file_name][1]}', con=engine, if_exists='fail', index=False)\n",
    "            counter += 1\n",
    "        except ValueError as e:\n",
    "            print(file_name + \"is present\")\n",
    "            print(e)\n",
    "            \n",
    "        print('Now cleaning up db')\n",
    "        del temp_df\n",
    "    print(f'Added {counter} databases')\n",
    "\n",
    "\n",
    "\n",
    "#   This will create file names that append the start year last 2 digits ie. 99 for 1999 and prefix DIET, DEMO, LAB, EXAM, QUEST for the respective filename. It will use the base file name ie. DEMO from DEMO_H.XPT as the filename\n",
    "def create_db_names(data_type, link_dict, file_dict):\n",
    "    \n",
    "#   Exludes a DEMO preview because single tables do not need DEMO_DEMO\n",
    "    prefix_dict = {'demographics': '', 'dietary': 'DIET_', 'examination': 'EXAM_', \n",
    "                   'laboratory': 'LAB_', 'questionnaire': 'QUEST_'}\n",
    "    \n",
    "    temp_dict = {}\n",
    "    \n",
    "#   Create temp_dict[filename:['2digit year']]\n",
    "    for link in xpt_link_dictionary[data_type]:\n",
    "            temp_dict[link.split('/')[-1]] = [link.split('/')[-2][2:4]]\n",
    "            \n",
    "#   Add prefix and DB name to temp_dict[xpt_filename: ['2digit year', 'DB Name example DIET_DSBI_99']]\n",
    "    for key, values in xpt_file_dictionary[data_type].items():\n",
    "        for value in values:\n",
    "            if len(value.split('_')) > 2:\n",
    "                #If there are multiple for same  year in sequence for instance lipids second value\n",
    "                temp_dict[value].append(f'{prefix_dict[data_type]}'+ value[:-6] + \"_\" + temp_dict[value][0])\n",
    "            else:\n",
    "                temp_dict[value].append(f'{prefix_dict[data_type]}'+ key + \"_\" + temp_dict[value][0])\n",
    "    \n",
    "    return temp_dict\n",
    "\n",
    "#Setting UTF8 and latin1 encoding errors \n",
    "#DSII does not play nice with UTF and encoding errors row '\\xC2\\x92S MU...' for column 'DSDSUPP' at row 74590\n",
    "#DSPI Incorrect string value: '\\xC2\\x92S MU...' for column 'DSDSUPP' at row 6913\n",
    "\n",
    "# Send folder of files and links to feather dataframes\n",
    "def send_to_feather(data_type, link_dict, file_dict):    \n",
    "    cwd = os.getcwd()\n",
    "    counter = 0\n",
    "    feather_name_dict = create_db_names(data_type, link_dict, file_dict)\n",
    "    for file in glob.glob(f'{cwd}/{data_type}/*'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(f\"Creating dataframe from {file}\")\n",
    "        temp_df = pd.read_sas(file)\n",
    "        print(f'Sending to Feather as {feather_name_dict[file_name][1]}')\n",
    "        try:\n",
    "            temp_df.to_feather(f'{cwd}/{data_type}_feather/{feather_name_dict[file_name][1]}.feather')\n",
    "            counter += 1\n",
    "        except ValueError as e:\n",
    "            print(file_name + \"is present\")\n",
    "            print(e)    \n",
    "        print('Now cleaning up dataframe')\n",
    "        del temp_df\n",
    "    print(f'Added {counter} feather dataframes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+mysqlconnector://tom:password@localhost:3306/nhanes', echo=False)\n",
    "send_to_db('tom', 'password', 'localhost','3306', 'nhanes', 'dietary', xpt_link_dictionary, xpt_file_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpt_link_dictionary = json.loads(open('xpt_link_dict.json').read())\n",
    "xpt_file_dictionary = json.loads(open('xpt_file_dict.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df99 = pd.read_sas(\"demographics/DEMO.XPT\")\n",
    "df01 = pd.read_sas('demographics/DEMO_B.XPT')\n",
    "df03 = pd.read_sas('demographics/DEMO_C.XPT')\n",
    "df05 = pd.read_sas('demographics/DEMO_D.XPT')\n",
    "df07 = pd.read_sas('demographics/DEMO_E.XPT')\n",
    "df09 = pd.read_sas('demographics/DEMO_F.XPT')\n",
    "df11 = pd.read_sas('demographics/DEMO_G.XPT')\n",
    "df13 = pd.read_sas('demographics/DEMO_H.XPT')\n",
    "df15 = pd.read_sas('demographics/DEMO_I.XPT')\n",
    "\n",
    "# Concat all demographic tables without sorting columns\n",
    "df99to15 = pd.concat([df99, df01, df03,df05,df07,df09,df11,df13,df15], sort=False)\n",
    "# Removed WTIRE and WTMREP 52 columns x2, left with 66 columns\n",
    "df99to15_cleaned = df99to15[df99to15.columns[~df99to15.columns.str.match('(WTIRE|WTMREP)')]]\n",
    "df99to15_clean = df99to15_cleaned.set_index('SEQN')\n",
    "df99to15_clean.to_csv('demographics_1999-2016.csv')\n",
    "\n",
    "df99to15_clean.reset_index().to_feather('demographics_feather/demographics_1999-2016.feather')\n",
    "send_to_feather('questionnaire', xpt_link_dictionary, xpt_file_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lipid profile sas readings\n",
    "\n",
    "hdl_07 = pd.read_sas('laboratory/HDL_E.XPT')\n",
    "hdl_05 = pd.read_sas('laboratory/HDL_D.XPT')\n",
    "hdl_09 = pd.read_sas('laboratory/HDL_F.XPT')\n",
    "hdl_11 = pd.read_sas('laboratory/HDL_G.XPT')\n",
    "hdl_13 = pd.read_sas('laboratory/HDL_H.XPT')\n",
    "hdl_15 = pd.read_sas('laboratory/HDL_I.XPT')\n",
    "ldl_07 = pd.read_sas('laboratory/TRIGLY_E.XPT')\n",
    "ldl_99 = pd.read_sas('laboratory/LAB13AM.XPT')\n",
    "ldl_01 = pd.read_sas('laboratory/L13AM_B.XPT')\n",
    "ldl_03 = pd.read_sas('laboratory/L13AM_C.XPT')\n",
    "ldl_09 = pd.read_sas('laboratory/TRIGLY_F.XPT')\n",
    "ldl_11 = pd.read_sas('laboratory/TRIGLY_G.XPT')\n",
    "ldl_13 = pd.read_sas('laboratory/TRIGLY_H.XPT')\n",
    "ldl_05 = pd.read_sas('laboratory/TRIGLY_D.XPT')\n",
    "total_07 = pd.read_sas('laboratory/TCHOL_E.XPT')\n",
    "total_05 = pd.read_sas('laboratory/TCHOL_D.XPT')\n",
    "total_09 = pd.read_sas('laboratory/TCHOL_F.XPT')\n",
    "total_11 = pd.read_sas('laboratory/TCHOL_G.XPT')\n",
    "total_13 = pd.read_sas('laboratory/TCHOL_H.XPT')\n",
    "total_15 = pd.read_sas('laboratory/TCHOL_I.XPT')\n",
    "total_99 = pd.read_sas(\"laboratory/LAB13.XPT\")\n",
    "total_01 = pd.read_sas(\"laboratory/L13_B.XPT\")\n",
    "total_03 = pd.read_sas(\"laboratory/L13_C.XPT\")\n",
    "apo_07 = pd.read_sas('laboratory/APOB_E.XPT')\n",
    "apo_09 = pd.read_sas('laboratory/APOB_F.XPT')\n",
    "apo_11 = pd.read_sas('laboratory/APOB_G.XPT')\n",
    "apo_13 = pd.read_sas('laboratory/APOB_H.XPT')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- The issues that need to be resolved, the total chol datasets from 99-05 have total and HDL cholesterol in both, but the rest are separated out. This needs to be mitigated by separating the total and hdl into two separate dfs and then concatting all years, and then merging the two in order to get a complete set.\n",
    "\n",
    "- For whatever reason, there is no LDL file on CDC site for 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some years with various columns names, this is to standardize columns names\n",
    "hdl_columns = {'LBDHDD':\"LBDHDL\", \"LBDHDDSI\":\"LBDHDLSI\"}\n",
    "renamed_hdl_df = pd.concat([hdl_05, hdl_07, hdl_09, hdl_11, hdl_13, hdl_15], sort=False).rename(columns=hdl_columns)\n",
    "total_03 = total_03.rename(columns={\"LBXHDD\":'LBDHDL', \"LBDHDDSI\":'LBDHDLSI'})\n",
    "total_df = pd.concat([total_05, total_07, total_09, total_11, total_13, total_15], sort=False)\n",
    "\n",
    "# First we need to separate the columns from the combined datasets into HDL specific and total chol specific dataframes, also separate the LDL from apo in 2005 dataframe\n",
    "hdl99to03 = pd.concat([total_99, total_01, total_03], sort=False).loc[:, ['SEQN', 'LBDHDL', 'LBDHDLSI']]\n",
    "total99to03 = pd.concat([total_99, total_01, total_03], sort=False).loc[:, ['SEQN', 'LBXTC', 'LBDTCSI']]\n",
    "ldl_only_05 = ldl_05.iloc[:,:-2]\n",
    "apo_05 = ldl_05.loc[:,['SEQN','WTSAF2YR','LBXAPB','LBDAPBSI']]\n",
    "\n",
    "#Then we combine all of the years to create separated datasets in preparation of merging\n",
    "hdl_complete_df = pd.concat([hdl99to03, renamed_hdl_df], sort=False)\n",
    "total_complete_df = pd.concat([total_df, total99to03], sort=False)\n",
    "ldl_complete_df = pd.concat([ldl_99, ldl_01, ldl_03, ldl_only_05, ldl_07, ldl_09, ldl_11, ldl_13], sort=False)\n",
    "apo_complete_df = pd.concat([apo_05, apo_07,apo_09, apo_11, apo_13], sort=False)\n",
    "\n",
    "\n",
    "lipid_complete_df = (hdl_complete_df\n",
    "                     .merge(total_complete_df, on='SEQN', how='outer')\n",
    "                     .merge(ldl_complete_df, on='SEQN', how='outer')\n",
    "                     .merge(apo_complete_df, on=['SEQN','WTSAF2YR'], how='outer').sort_values('SEQN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>LBDHDL</th>\n",
       "      <th>LBDHDLSI</th>\n",
       "      <th>LBXTC</th>\n",
       "      <th>LBDTCSI</th>\n",
       "      <th>WTSAF2YR</th>\n",
       "      <th>WTSAF4YR</th>\n",
       "      <th>LBXTR</th>\n",
       "      <th>LBDTRSI</th>\n",
       "      <th>LBDLDL</th>\n",
       "      <th>LBDLDLSI</th>\n",
       "      <th>LBXAPB</th>\n",
       "      <th>LBDAPBSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>215.0</td>\n",
       "      <td>5.56</td>\n",
       "      <td>60586.147294</td>\n",
       "      <td>33073.267573</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>136.0</td>\n",
       "      <td>3.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>129.0</td>\n",
       "      <td>3.34</td>\n",
       "      <td>121969.841152</td>\n",
       "      <td>52434.225472</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>279.0</td>\n",
       "      <td>7.21</td>\n",
       "      <td>234895.205650</td>\n",
       "      <td>98468.806492</td>\n",
       "      <td>347.0</td>\n",
       "      <td>3.92</td>\n",
       "      <td>168.0</td>\n",
       "      <td>4.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.57</td>\n",
       "      <td>153.0</td>\n",
       "      <td>3.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>245.0</td>\n",
       "      <td>6.34</td>\n",
       "      <td>57661.621988</td>\n",
       "      <td>32935.874064</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>127.0</td>\n",
       "      <td>3.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN  LBDHDL  LBDHDLSI  LBXTC  LBDTCSI       WTSAF2YR      WTSAF4YR  LBXTR  \\\n",
       "0   2.0    54.0      1.39  215.0     5.56   60586.147294  33073.267573  128.0   \n",
       "1   3.0    30.0      0.78  129.0     3.34  121969.841152  52434.225472  202.0   \n",
       "2   5.0    42.0      1.08  279.0     7.21  234895.205650  98468.806492  347.0   \n",
       "3   6.0    61.0      1.57  153.0     3.96            NaN           NaN    NaN   \n",
       "4   7.0   105.0      2.73  245.0     6.34   57661.621988  32935.874064   62.0   \n",
       "\n",
       "   LBDTRSI  LBDLDL  LBDLDLSI  LBXAPB  LBDAPBSI  \n",
       "0     1.45   136.0      3.52     NaN       NaN  \n",
       "1     2.28    58.0      1.50     NaN       NaN  \n",
       "2     3.92   168.0      4.34     NaN       NaN  \n",
       "3      NaN     NaN       NaN     NaN       NaN  \n",
       "4     0.70   127.0      3.28     NaN       NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipid_complete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_99 = pd.read_sas('laboratory/LAB25.XPT')\n",
    "cbc_01 = pd.read_sas('laboratory/L25_B.XPT')\n",
    "cbc_03 = pd.read_sas('laboratory/L25_C.XPT')\n",
    "cbc_05 = pd.read_sas('laboratory/CBC_D.XPT')\n",
    "cbc_07 = pd.read_sas('laboratory/CBC_E.XPT')\n",
    "cbc_09 = pd.read_sas('laboratory/CBC_F.XPT')\n",
    "cbc_11 = pd.read_sas('laboratory/CBC_G.XPT')\n",
    "cbc_13 = pd.read_sas('laboratory/CBC_H.XPT')\n",
    "cbc_15 = pd.read_sas('laboratory/CBC_I.XPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_15 = cbc_15.rename(columns={'LBXMCHSI':'LBXMC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_15 = cbc_15.drop(columns='LBXMCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_complete_df = pd.concat([cbc_99, cbc_01, cbc_03, cbc_05, \n",
    "                             cbc_07, cbc_09, cbc_11, cbc_13, cbc_15], sort=False).sort_values('SEQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
