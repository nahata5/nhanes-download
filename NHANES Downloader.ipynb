{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from time import gmtime, strftime, localtime\n",
    "import glob\n",
    "from sqlalchemy import create_engine\n",
    "# import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cdc_url = 'https://wwwn.cdc.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_links(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    table = soup.find(lambda tag: tag.has_attr('id') and tag['id']==\"GridView1\")\n",
    "    \n",
    "# Lambda expression for all links that end with XPT\n",
    "    link_list = table.findAll(lambda tag: tag.name=='a' and tag['href'].endswith(\".XPT\"))\n",
    "    links_only = [link.get('href') for link in link_list]\n",
    "    \n",
    "    return links_only\n",
    "\n",
    "# This gets all of the links for the multiple years of data listed in year_list in order to batch download files\n",
    "def get_multi_year(data_type, base_url):\n",
    "    datatype_dict = {'demographics':'Demographics', 'dietary':'Dietary',\n",
    "                     'examination':'Examination', 'laboratory':'Laboratory', \n",
    "                     'questionnaire':'Questionnaire'}\n",
    "    # Can add years as future years are added\n",
    "    year_list = [1999, 2001, 2003, 2005, 2007, 2009, 2011, 2013, 2015]\n",
    "    data_links = []\n",
    "    for year in year_list:\n",
    "        url = f\"{base_url}/nchs/nhanes/search/datapage.aspx?Component={datatype_dict[data_type]}&CycleBeginYear={year}\"\n",
    "        temp_data_links = get_table_links(url)\n",
    "        for data in temp_data_links:\n",
    "            if data not in data_links:\n",
    "                data_links.append(data)\n",
    "                print(f\"Added {data} from {year}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    return data_links\n",
    "\n",
    "# Can use the link of filename.htm on top of base_cdc_url for access to the codebook links. This will produce a dictionary of column names can replace\n",
    "def get_column_labels(xpt_link_name, base_url):\n",
    "    htm_filename = f'{xpt_link_name[:-3]}htm'\n",
    "    r = requests.get(f'{base_url}{htm_filename}')\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    # Codebook section of documentation\n",
    "    # TODO -- take section or pdf htm pages\n",
    "    codebook_links = soup.findAll('div', id='CodebookLinks')[0].findAll('a')\n",
    "    \n",
    "    dictionary = {link.string.split('-')[0].strip() : link.string.split('-')[1].strip() for link in codebook_links}\n",
    "    return dictionary\n",
    "\n",
    "# Batch download function based off of data_type ['demographics', 'examination', 'dietary', 'laboratory', 'questionnaire']\n",
    "def download_data(data_type, link_list, base_url):\n",
    "    cwd = os.getcwd()\n",
    "    try:\n",
    "        os.mkdir(data_type)\n",
    "        print(f'Created {data_type} folder')\n",
    "    except:\n",
    "        print(f'{data_type} folder exists')\n",
    "    for link in link_list:\n",
    "        item_name = link.split('/')[-1]\n",
    "        exists = os.path.isfile(f'{cwd}/{data_type}/{item_name}')\n",
    "        if exists:\n",
    "            print(f'{item_name} already exists')\n",
    "        else:\n",
    "            current_time = time.time()\n",
    "            print(f'Downloading {item_name} at {strftime(\"%a, %d %b %Y %H:%M:%S\", localtime())}')\n",
    "            r = requests.get(base_url + link, allow_redirects=True)\n",
    "            open(f'{cwd}/{data_type}/{item_name}', 'wb').write(r.content)\n",
    "            time_elapsed = time.time() - current_time\n",
    "            print(f'Downloaded {item_name} at {time_elapsed}s')\n",
    "\n",
    "# Create a dictionary of filenames for database\n",
    "def create_xpt_dict(data_type):\n",
    "    original_file_names = {}\n",
    "    group_file_names = []\n",
    "    for file in glob.glob(f'{data_type}/*'):\n",
    "        xpt_file = file.split('/')[1]\n",
    "        if len(xpt_file.split('_'))== 1:\n",
    "            original_file_names[xpt_file.split('.')[0]] = [xpt_file]\n",
    "    for file in glob.glob(f'{data_type}/*'):\n",
    "        xpt_file = file.split('/')[1]\n",
    "        if len(xpt_file.split('_'))> 1:\n",
    "            try:\n",
    "                xpt_name = xpt_file.split('_')[0]\n",
    "                original_file_names[f'{xpt_name}'].append(xpt_file)\n",
    "            except KeyError as e:\n",
    "                xpt_name = xpt_file.split('_')[0]\n",
    "                original_file_names[f'{xpt_name}'] = [xpt_file]               \n",
    "    return original_file_names\n",
    "        \n",
    "# Concat tables based on file names\n",
    "def combine_tables(data_type, xpt_dict):\n",
    "    temp_df_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for keys, values in xpt_dict[data_type].items():\n",
    "        for value in values:\n",
    "            print(f'Trying {cwd}/{data_type}/{value}')\n",
    "            temp_df_list.append(pd.read_sas(f'{cwd}/{data_type}/{value}'))\n",
    "            print(f'{cwd}/{data_type}/{value} appended')\n",
    "    return pd.concat(temp_df_list)\n",
    "                  \n",
    "# Error handling if downloading empty files\n",
    "def grab_empty_files(data_type, base_url):\n",
    "    empty_list = []\n",
    "    cwd = os.getcwd()\n",
    "    for file in glob.glob(f'{cwd}/{data_type}/*'):\n",
    "        if os.stat(file).st_size == 0:\n",
    "            empty_list.append(file)\n",
    "            os.remove(file)\n",
    "    if len(empty_list) == 0:\n",
    "        print(\"There are no empty files in this folder\")\n",
    "    else:\n",
    "        print(f\"Now re-downloading {len(empty_list)} files\")\n",
    "        download_data(data_type, empty_list, base_url)\n",
    "\n",
    "# demographic_links = get_multi_year('demographics', base_cdc_url)\n",
    "# dietary_links = get_multi_year('dietary', base_cdc_url)\n",
    "# examination_links = get_multi_year('examination', base_cdc_url)\n",
    "# laboratory_links = get_multi_year('laboratory', base_cdc_url)\n",
    "# questionnaire_links = get_multi_year('questionnaire', base_cdc_url)\n",
    "\n",
    "# link_dictionary = {'demographics':demographic_links, 'dietary':dietary_links, \n",
    "#                    'examination':examination_links, 'laboratory':laboratory_links,\n",
    "#                   'questionnaire':questionnaire_links}\n",
    "# with open('xpt_link_dict.json', 'w') as f:\n",
    "#     json.dump(link_dictionary, f)\n",
    "    \n",
    "# Create the xpt_file_dict json for individual table creation and anticipation of merged tabes\n",
    "# xpt_file_dict = {}\n",
    "# for keys in link_dictionary:\n",
    "#     xpt_file_dict[keys] = create_xpt_dict(keys)\n",
    "    \n",
    "# with open('xpt_file_dict.json', 'w') as f:\n",
    "#     json.dump(xpt_file_dict, f)\n",
    "    \n",
    "\n",
    "    \n",
    "# # Download data - \n",
    "# download_data('demographics', xpt_link_dictionary['demographics'], base_cdc_url)\n",
    "# download_data('dietary', xpt_link_dictionary['dietary'], base_cdc_url)\n",
    "# download_data('examination', xpt_link_dictionary['examination'], base_cdc_url)\n",
    "# download_data('laboratory', xpt_link_dictionary['laboratory'], base_cdc_url)\n",
    "# download_data('questionnaire', xpt_link_dictionary['questionnaire'], base_cdc_url)\n",
    "\n",
    "\n",
    "#Ensure DB max_allowed_packet is set to 1G, this funciton will send to a mysql database\n",
    "def send_to_db(user,password,host,port,database, data_type, link_dict, file_dict):\n",
    "    engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}:{port}/{database}', \n",
    "                           echo=False)\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    counter = 0\n",
    "    db_name_dict = create_db_names(data_type, link_dict, file_dict)\n",
    "    for file in glob.glob(f'{cwd}/{data_type}/*'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(f\"Creating dataframe from {file}\")\n",
    "        temp_df = pd.read_sas(file, encoding='ISO-8859-1')\n",
    "        print(f'Sending to MySQL Server as {db_name_dict[file_name][1]}')\n",
    "        try:\n",
    "            temp_df.to_sql(name=f'{db_name_dict[file_name][1]}', con=engine, if_exists='fail', index=False)\n",
    "            counter += 1\n",
    "        except ValueError as e:\n",
    "            print(file_name + \"is present\")\n",
    "            print(e)\n",
    "            \n",
    "        print('Now cleaning up db')\n",
    "        del temp_df\n",
    "    print(f'Added {counter} databases')\n",
    "\n",
    "\n",
    "\n",
    "#   This will create file names that append the start year last 2 digits ie. 99 for 1999 and prefix DIET, DEMO, LAB, EXAM, QUEST for the respective filename. It will use the base file name ie. DEMO from DEMO_H.XPT as the filename\n",
    "def create_db_names(data_type, link_dict, file_dict):\n",
    "    \n",
    "#   Exludes a DEMO preview because single tables do not need DEMO_DEMO\n",
    "    prefix_dict = {'demographics': '', 'dietary': 'DIET_', 'examination': 'EXAM_', \n",
    "                   'laboratory': 'LAB_', 'questionnaire': 'QUEST_'}\n",
    "    \n",
    "    temp_dict = {}\n",
    "    \n",
    "#   Create temp_dict[filename:['2digit year']]\n",
    "    for link in xpt_link_dictionary[data_type]:\n",
    "            temp_dict[link.split('/')[-1]] = [link.split('/')[-2][2:4]]\n",
    "            \n",
    "#   Add prefix and DB name to temp_dict[xpt_filename: ['2digit year', 'DB Name example DIET_DSBI_99']]\n",
    "    for key, values in xpt_file_dictionary[data_type].items():\n",
    "        for value in values:\n",
    "            if len(value.split('_')) > 2:\n",
    "                #If there are multiple for same  year in sequence for instance lipids second value\n",
    "                temp_dict[value].append(f'{prefix_dict[data_type]}'+ value[:-6] + \"_\" + temp_dict[value][0])\n",
    "            else:\n",
    "                temp_dict[value].append(f'{prefix_dict[data_type]}'+ key + \"_\" + temp_dict[value][0])\n",
    "    \n",
    "    return temp_dict\n",
    "\n",
    "#Setting UTF8 and latin1 encoding errors \n",
    "#DSII does not play nice with UTF and encoding errors row '\\xC2\\x92S MU...' for column 'DSDSUPP' at row 74590\n",
    "#DSPI Incorrect string value: '\\xC2\\x92S MU...' for column 'DSDSUPP' at row 6913\n",
    "\n",
    "# Send folder of files and links to feather dataframes\n",
    "def send_to_feather(data_type, link_dict, file_dict):    \n",
    "    cwd = os.getcwd()\n",
    "    counter = 0\n",
    "    feather_name_dict = create_db_names(data_type, link_dict, file_dict)\n",
    "    for file in glob.glob(f'{cwd}/{data_type}/*'):\n",
    "        file_name = file.split('/')[-1]\n",
    "        print(f\"Creating dataframe from {file}\")\n",
    "        temp_df = pd.read_sas(file)\n",
    "        print(f'Sending to Feather as {feather_name_dict[file_name][1]}')\n",
    "        try:\n",
    "            temp_df.to_feather(f'{cwd}/{data_type}_feather/{feather_name_dict[file_name][1]}.feather')\n",
    "            counter += 1\n",
    "        except ValueError as e:\n",
    "            print(file_name + \"is present\")\n",
    "            print(e)    \n",
    "        print('Now cleaning up dataframe')\n",
    "        del temp_df\n",
    "    print(f'Added {counter} feather dataframes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+mysqlconnector://tom:password@localhost:3306/nhanes', echo=False)\n",
    "send_to_db('tom', 'password', 'localhost','3306', 'nhanes', 'dietary', xpt_link_dictionary, xpt_file_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpt_link_dictionary = json.loads(open('xpt_link_dict.json').read())\n",
    "xpt_file_dictionary = json.loads(open('xpt_file_dict.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df99 = pd.read_sas(\"demographics/DEMO.XPT\")\n",
    "df01 = pd.read_sas('demographics/DEMO_B.XPT')\n",
    "df03 = pd.read_sas('demographics/DEMO_C.XPT')\n",
    "df05 = pd.read_sas('demographics/DEMO_D.XPT')\n",
    "df07 = pd.read_sas('demographics/DEMO_E.XPT')\n",
    "df09 = pd.read_sas('demographics/DEMO_F.XPT')\n",
    "df11 = pd.read_sas('demographics/DEMO_G.XPT')\n",
    "df13 = pd.read_sas('demographics/DEMO_H.XPT')\n",
    "df15 = pd.read_sas('demographics/DEMO_I.XPT')\n",
    "# Add years\n",
    "df99.insert(1, 'STARTYEAR','1999')\n",
    "df99.insert(2,'ENDYEAR','2001')\n",
    "df01.insert(1, 'STARTYEAR','2001')\n",
    "df01.insert(2,'ENDYEAR','2002')\n",
    "df03.insert(1, 'STARTYEAR','2003')\n",
    "df03.insert(2,'ENDYEAR','2004')\n",
    "df05.insert(1, 'STARTYEAR','2005')\n",
    "df05.insert(2,'ENDYEAR','2006')\n",
    "df07.insert(1, 'STARTYEAR','2007')\n",
    "df07.insert(2,'ENDYEAR','2008')\n",
    "df09.insert(1, 'STARTYEAR','2009')\n",
    "df09.insert(2,'ENDYEAR','2010')\n",
    "df11.insert(1, 'STARTYEAR','2011')\n",
    "df11.insert(2,'ENDYEAR','2012')\n",
    "df13.insert(1, 'STARTYEAR','2013')\n",
    "df13.insert(2,'ENDYEAR','2014')\n",
    "df15.insert(1, 'STARTYEAR','2015')\n",
    "df15.insert(2,'ENDYEAR','2016')\n",
    "\n",
    "# Concat all demographic tables without sorting columns\n",
    "df99to15 = pd.concat([df99, df01, df03,df05,df07,df09,df11,df13,df15], sort=False)\n",
    "# Removed WTIRE and WTMREP 52 columns x2, left with 66 columns\n",
    "# df99to15_cleaned = df99to15[df99to15.columns[~df99to15.columns.str.match('(WTIRE|WTMREP)')]]\n",
    "# df99to15_clean = df99to15_cleaned.set_index('SEQN')\n",
    "# df99to15_clean.to_csv('demographics_1999-2016.csv')\n",
    "\n",
    "# df99to15_clean.reset_index().to_feather('demographics_feather/demographics_1999-2016.feather')\n",
    "# send_to_feather('questionnaire', xpt_link_dictionary, xpt_file_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqn_year_df = df99to15.loc[:,['SEQN','STARTYEAR','ENDYEAR']]\n",
    "seqn_year_df.to_csv('SEQNYEAR99to16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lipid profile sas readings\n",
    "\n",
    "hdl_07 = pd.read_sas('laboratory/HDL_E.XPT')\n",
    "hdl_05 = pd.read_sas('laboratory/HDL_D.XPT')\n",
    "hdl_09 = pd.read_sas('laboratory/HDL_F.XPT')\n",
    "hdl_11 = pd.read_sas('laboratory/HDL_G.XPT')\n",
    "hdl_13 = pd.read_sas('laboratory/HDL_H.XPT')\n",
    "hdl_15 = pd.read_sas('laboratory/HDL_I.XPT')\n",
    "ldl_07 = pd.read_sas('laboratory/TRIGLY_E.XPT')\n",
    "ldl_99 = pd.read_sas('laboratory/LAB13AM.XPT')\n",
    "ldl_01 = pd.read_sas('laboratory/L13AM_B.XPT')\n",
    "ldl_03 = pd.read_sas('laboratory/L13AM_C.XPT')\n",
    "ldl_09 = pd.read_sas('laboratory/TRIGLY_F.XPT')\n",
    "ldl_11 = pd.read_sas('laboratory/TRIGLY_G.XPT')\n",
    "ldl_13 = pd.read_sas('laboratory/TRIGLY_H.XPT')\n",
    "ldl_05 = pd.read_sas('laboratory/TRIGLY_D.XPT')\n",
    "total_07 = pd.read_sas('laboratory/TCHOL_E.XPT')\n",
    "total_05 = pd.read_sas('laboratory/TCHOL_D.XPT')\n",
    "total_09 = pd.read_sas('laboratory/TCHOL_F.XPT')\n",
    "total_11 = pd.read_sas('laboratory/TCHOL_G.XPT')\n",
    "total_13 = pd.read_sas('laboratory/TCHOL_H.XPT')\n",
    "total_15 = pd.read_sas('laboratory/TCHOL_I.XPT')\n",
    "total_99 = pd.read_sas(\"laboratory/LAB13.XPT\")\n",
    "total_01 = pd.read_sas(\"laboratory/L13_B.XPT\")\n",
    "total_03 = pd.read_sas(\"laboratory/L13_C.XPT\")\n",
    "apo_07 = pd.read_sas('laboratory/APOB_E.XPT')\n",
    "apo_09 = pd.read_sas('laboratory/APOB_F.XPT')\n",
    "apo_11 = pd.read_sas('laboratory/APOB_G.XPT')\n",
    "apo_13 = pd.read_sas('laboratory/APOB_H.XPT')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- The issues that need to be resolved, the total chol datasets from 99-05 have total and HDL cholesterol in both, but the rest are separated out. This needs to be mitigated by separating the total and hdl into two separate dfs and then concatting all years, and then merging the two in order to get a complete set.\n",
    "\n",
    "- For whatever reason, there is no LDL file on CDC site for 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some years with various columns names, this is to standardize columns names\n",
    "hdl_columns = {'LBDHDD':\"LBDHDL\", \"LBDHDDSI\":\"LBDHDLSI\"}\n",
    "renamed_hdl_df = pd.concat([hdl_05, hdl_07, hdl_09, hdl_11, hdl_13, hdl_15], sort=False).rename(columns=hdl_columns)\n",
    "total_03 = total_03.rename(columns={\"LBXHDD\":'LBDHDL', \"LBDHDDSI\":'LBDHDLSI'})\n",
    "total_df = pd.concat([total_05, total_07, total_09, total_11, total_13, total_15], sort=False)\n",
    "\n",
    "# First we need to separate the columns from the combined datasets into HDL specific and total chol specific dataframes, also separate the LDL from apo in 2005 dataframe\n",
    "hdl99to03 = pd.concat([total_99, total_01, total_03], sort=False).loc[:, ['SEQN', 'LBDHDL', 'LBDHDLSI']]\n",
    "total99to03 = pd.concat([total_99, total_01, total_03], sort=False).loc[:, ['SEQN', 'LBXTC', 'LBDTCSI']]\n",
    "ldl_only_05 = ldl_05.iloc[:,:-2]\n",
    "apo_05 = ldl_05.loc[:,['SEQN','WTSAF2YR','LBXAPB','LBDAPBSI']]\n",
    "\n",
    "#Then we combine all of the years to create separated datasets in preparation of merging\n",
    "hdl_complete_df = pd.concat([hdl99to03, renamed_hdl_df], sort=False)\n",
    "total_complete_df = pd.concat([total_df, total99to03], sort=False)\n",
    "ldl_complete_df = pd.concat([ldl_99, ldl_01, ldl_03, ldl_only_05, ldl_07, ldl_09, ldl_11, ldl_13], sort=False)\n",
    "apo_complete_df = pd.concat([apo_05, apo_07,apo_09, apo_11, apo_13], sort=False)\n",
    "\n",
    "\n",
    "lipid_complete_df = (hdl_complete_df\n",
    "                     .merge(total_complete_df, on='SEQN', how='outer')\n",
    "                     .merge(ldl_complete_df, on='SEQN', how='outer')\n",
    "                     .merge(apo_complete_df, on=['SEQN','WTSAF2YR'], how='outer').sort_values('SEQN'))\n",
    "lipid_complete_df.to_csv('LipidProfile99to16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CBC for all years 99 to 15\n",
    "cbc_99 = pd.read_sas('laboratory/LAB25.XPT')\n",
    "cbc_01 = pd.read_sas('laboratory/L25_B.XPT')\n",
    "cbc_03 = pd.read_sas('laboratory/L25_C.XPT')\n",
    "cbc_05 = pd.read_sas('laboratory/CBC_D.XPT')\n",
    "cbc_07 = pd.read_sas('laboratory/CBC_E.XPT')\n",
    "cbc_09 = pd.read_sas('laboratory/CBC_F.XPT')\n",
    "cbc_11 = pd.read_sas('laboratory/CBC_G.XPT')\n",
    "cbc_13 = pd.read_sas('laboratory/CBC_H.XPT')\n",
    "cbc_15 = pd.read_sas('laboratory/CBC_I.XPT')\n",
    "\n",
    "# Need to rename and remove two columns in year 15, LBXMCHSI is in pg not in g/dl which is what it used to be in LBXMC. LBXMCH is now a new column that only has NAN data\n",
    "cbc_15 = cbc_15.rename(columns={'LBXMCHSI':'LBXMC'})\n",
    "cbc_15 = cbc_15.drop(columns='LBXMCH')\n",
    "\n",
    "#Create complete cbc df\n",
    "cbc_complete_df = pd.concat([cbc_99, cbc_01, cbc_03, cbc_05, \n",
    "                             cbc_07, cbc_09, cbc_11, cbc_13, cbc_15], sort=False).sort_values('SEQN')\n",
    "cbc_complete_df = cbc_complete_df.merge(seqn_year_df, on='SEQN', how='outer')\n",
    "cbc_complete_df.to_csv('CBC99to16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_cr_99 = pd.read_sas('laboratory/LAB16.XPT')\n",
    "alb_cr_01 = pd.read_sas('laboratory/L16_B.XPT')\n",
    "alb_cr_03 = pd.read_sas('laboratory/L16_C.XPT')\n",
    "alb_cr_05 = pd.read_sas('laboratory/ALB_CR_D.XPT')\n",
    "alb_cr_07 = pd.read_sas('laboratory/ALB_CR_E.XPT')\n",
    "alb_cr_09 = pd.read_sas('laboratory/ALB_CR_F.XPT')\n",
    "alb_cr_11 = pd.read_sas('laboratory/ALB_CR_G.XPT')\n",
    "alb_cr_13 = pd.read_sas('laboratory/ALB_CR_H.XPT')\n",
    "alb_cr_15 = pd.read_sas('laboratory/ALB_CR_I.XPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to transform dataframes up to 07 to add calculated urine albumin/cr ratio, it was pre-calculated after 09\n",
    "# Need to multiply by  100 to change units from ug to mg URDACT = URXUMA/URXUCR x 100\n",
    "# 2009 was the only year with a second collection 10 days after initial collection\n",
    "# Therefore, the 2nd collection as like other dataframes was removed to remain consistent\n",
    "alb_cr_99.loc[:, 'URDACT'] = alb_cr_99.loc[:,'URXUMA'] * 100/ alb_cr_99.loc[:, 'URXUCR']\n",
    "alb_cr_01.loc[:, 'URDACT'] = alb_cr_01.loc[:,'URXUMA'] * 100/ alb_cr_01.loc[:, 'URXUCR']\n",
    "alb_cr_03.loc[:, 'URDACT'] = alb_cr_03.loc[:,'URXUMA'] * 100/ alb_cr_03.loc[:, 'URXUCR']\n",
    "alb_cr_05.loc[:, 'URDACT'] = alb_cr_05.loc[:,'URXUMA'] * 100/ alb_cr_05.loc[:, 'URXUCR']\n",
    "alb_cr_07.loc[:, 'URDACT'] = alb_cr_07.loc[:,'URXUMA'] * 100/ alb_cr_07.loc[:, 'URXUCR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2009 there needs to remove 2nd collection columns, please refer to original file if seeking\n",
    "alb_cr_09_trim = alb_cr_09.iloc[:, :-5]\n",
    "# Beginning in 2005, new naming convention for the SI, standardize names of prior columns as below\n",
    "alb_rename = {\"URXUMASI\":'URXUMS', 'URXUCRSI':'URXCRS'}\n",
    "alb_cr_99 = alb_cr_99.rename(columns=alb_rename)\n",
    "alb_cr_01 = alb_cr_01.rename(columns=alb_rename)\n",
    "alb_cr_03 = alb_cr_01.rename(columns=alb_rename)\n",
    "\n",
    "# Removing the \"lower limit of detection columns\" in 2015 data, doesn't exist in any other set\n",
    "alb_cr_15_trim = alb_cr_15.loc[:,['SEQN', 'URXUMA', 'URXUMS', 'URXUCR','URXCRS', 'URDACT']]\n",
    "alb_dfs = [alb_cr_99, alb_cr_01,alb_cr_03, alb_cr_05,\n",
    "           alb_cr_07,alb_cr_09_trim,alb_cr_11,alb_cr_13,alb_cr_15_trim]\n",
    "alb_cr_complete_df = pd.concat(alb_dfs, sort=False)\n",
    "alb_cr_complete_df = alb_cr_complete_df.merge(seqn_year_df, on='SEQN', how='outer')\n",
    "# IF needed can use to recreate csv\n",
    "# alb_cr_complete_df.to_csv('UrineCrAlbumin99to16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biochemistry metobolic panel\n",
    "cmp_99 = pd.read_sas('laboratory/LAB18.XPT')\n",
    "cmp_01 = pd.read_sas('laboratory/L40_B.XPT')\n",
    "cmp_03 = pd.read_sas('laboratory/L40_C.XPT')\n",
    "cmp_05 = pd.read_sas('laboratory/BIOPRO_D.XPT')\n",
    "cmp_07 = pd.read_sas('laboratory/BIOPRO_E.XPT')\n",
    "cmp_09 = pd.read_sas('laboratory/BIOPRO_F.XPT')\n",
    "cmp_11 = pd.read_sas('laboratory/BIOPRO_G.XPT')\n",
    "cmp_13 = pd.read_sas('laboratory/BIOPRO_H.XPT')\n",
    "cmp_15 = pd.read_sas('laboratory/BIOPRO_I.XPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqn_year_df = pd.read_csv('SEQNYEAR99to16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to remove hormone columns, should be in separate DF\n",
    "cmp_99 = cmp_99.iloc[:, :-4]\n",
    "cmp_01 = cmp_01.iloc[:, :-4]\n",
    "# Correct 99-00 serum Cr according to national kidney disease education program to standardize these\n",
    "cmp_99['LBXSCR'] = cmp_99.LBXSCR *1.013 + 0.147\n",
    "rename01cmp_dict = {'LBDSAPSI':'LBXSAPSI', 'LBDSLDSI':'LBXSLDSI', \n",
    "                    'LBDSPH':'LBXSPH', 'LBDSTB':'LBXSTB', 'LBDSCR':'LBXSCR'}\n",
    "cmp_01 = cmp_01.rename(columns=rename01cmp_dict)\n",
    "# --TODO -- \n",
    "# Determine if triglycerides should be included as separate, different column names\n",
    "# Correct serum creatine for 99-00  Standard Creatinine (Y) = 1.013*NHANES Creatinine (X) + 0.147 (r = 0.984 \n",
    "\n",
    "complete_biochem_df = pd.concat([cmp_99, cmp_01, cmp_03, cmp_05, \n",
    "                                 cmp_07, cmp_09, cmp_11, cmp_13, cmp_15], sort=False)\n",
    "complete_biochem_df = complete_biochem_df.merge(seqn_year_df, on='SEQN', how='outer')\n",
    "# complete_biochem_df.to_csv('StandardBiochemPanel99to16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hemoglobin A1C\n",
    "a1c_99 = pd.read_sas('laboratory/LAB10.XPT')\n",
    "a1c_01 = pd.read_sas('laboratory/L10_B.XPT')\n",
    "a1c_03 = pd.read_sas('laboratory/L10_C.XPT')\n",
    "a1c_05 = pd.read_sas('laboratory/GHB_D.XPT')\n",
    "a1c_07 = pd.read_sas('laboratory/GHB_E.XPT')\n",
    "a1c_09 = pd.read_sas('laboratory/GHB_F.XPT')\n",
    "a1c_11 = pd.read_sas('laboratory/GHB_G.XPT')\n",
    "a1c_13 = pd.read_sas('laboratory/GHB_H.XPT')\n",
    "a1c_15 = pd.read_sas('laboratory/GHB_I.XPT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1c_complete_df = pd.concat([a1c_99,a1c_01,a1c_03,a1c_05,a1c_07,a1c_09,a1c_11,a1c_13,a1c_15])\n",
    "a1c_complete_df = a1c_complete_df.merge(seqn_year_df, on='SEQN', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1c_complete_df.to_csv('HGBA1C99to16', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>LBXGH</th>\n",
       "      <th>STARTYEAR</th>\n",
       "      <th>ENDYEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>62731.000000</td>\n",
       "      <td>58907.000000</td>\n",
       "      <td>62731.000000</td>\n",
       "      <td>62731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>46430.254738</td>\n",
       "      <td>5.578447</td>\n",
       "      <td>2006.937925</td>\n",
       "      <td>2008.045655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>27017.357140</td>\n",
       "      <td>0.987572</td>\n",
       "      <td>5.139797</td>\n",
       "      <td>4.980292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>2001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23175.500000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2003.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46140.000000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>69035.500000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>93702.000000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEQN         LBXGH     STARTYEAR       ENDYEAR\n",
       "count  62731.000000  58907.000000  62731.000000  62731.000000\n",
       "mean   46430.254738      5.578447   2006.937925   2008.045655\n",
       "std    27017.357140      0.987572      5.139797      4.980292\n",
       "min        2.000000      2.000000   1999.000000   2001.000000\n",
       "25%    23175.500000      5.100000   2003.000000   2004.000000\n",
       "50%    46140.000000      5.400000   2007.000000   2008.000000\n",
       "75%    69035.500000      5.700000   2011.000000   2012.000000\n",
       "max    93702.000000     18.800000   2015.000000   2016.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1c_complete_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clinicprep/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "# Hepatitis Panel\n",
    "#Hep a\n",
    "hav_99 = pd.read_sas('laboratory/L02HPA_A.XPT')\n",
    "hav_01 = pd.read_sas('laboratory/L02HPA_B.XPT')\n",
    "hav_03 = pd.read_sas('laboratory/L02HPA_C.XPT')\n",
    "hav_05 = pd.read_sas('laboratory/HEPA_D.XPT')\n",
    "hav_07 = pd.read_sas('laboratory/HEPA_E.XPT')\n",
    "hav_09 = pd.read_sas('laboratory/HEPA_F.XPT')\n",
    "hav_11 = pd.read_sas('laboratory/HEPA_G.XPT')\n",
    "hav_13 = pd.read_sas('laboratory/HEPA_H.XPT')\n",
    "hav_15 = pd.read_sas('laboratory/HEPA_I.XPT')\n",
    "\n",
    "#HBSAB\n",
    "hbsab_99 = pd.read_sas('laboratory/L02HBS.XPT')\n",
    "hbsab_01 = pd.read_sas('laboratory/L02HBS_B.XPT')\n",
    "hbsab_03 = pd.read_sas('laboratory/L02HBS_C.XPT')\n",
    "hbsab_05 = pd.read_sas('laboratory/HEPB_S_D.XPT')\n",
    "hbsab_07 = pd.read_sas('laboratory/HEPB_S_E.XPT')\n",
    "hbsab_09 = pd.read_sas('laboratory/HEPB_S_F.XPT')\n",
    "hbsab_11 = pd.read_sas('laboratory/HEPB_S_G.XPT')\n",
    "hbsab_13 = pd.read_sas('laboratory/HEPB_S_H.XPT')\n",
    "hbsab_15 = pd.read_sas('laboratory/HEPB_S_I.XPT')\n",
    "\n",
    "#HBCore and SAg - 99, 01, 03 contain HCV ab and rna\n",
    "hbsag_99 = pd.read_sas('laboratory/LAB02.XPT')\n",
    "hbsag_01 = pd.read_sas('laboratory/L02_B.XPT')\n",
    "hbsag_03 = pd.read_sas('laboratory/L02_C.XPT')\n",
    "hbsag_05 = pd.read_sas('laboratory/HEPBD_D.XPT')\n",
    "hbsag_07 = pd.read_sas('laboratory/HEPBD_E.XPT')\n",
    "hbsag_09 = pd.read_sas('laboratory/HEPBD_F.XPT')\n",
    "hbsag_11 = pd.read_sas('laboratory/HEPBD_G.XPT')\n",
    "hbsag_13 = pd.read_sas('laboratory/HEPBD_H.XPT')\n",
    "hbsag_15 = pd.read_sas('laboratory/HEPBD_I.XPT')\n",
    "\n",
    "#HCV - will need to extract and combine hb files\n",
    "hcv_99 = hbsag_99.loc[:,['SEQN','LBDHCV','LBXHCR']]\n",
    "hcv_01 = hbsag_01.loc[:,['SEQN','LBDHCV','LBXHCR']]\n",
    "hcv_03 = hbsag_03.loc[:,['SEQN','LBDHCV','LBXHCR']]\n",
    "hcv_05 = pd.read_sas('laboratory/HEPC_D.XPT')\n",
    "hcv_07 = pd.read_sas('laboratory/HEPC_E.XPT')\n",
    "hcv_09 = pd.read_sas('laboratory/HEPC_F.XPT')\n",
    "hcv_11 = pd.read_sas('laboratory/HEPC_G.XPT')\n",
    "hcv_13 = pd.read_sas('laboratory/HEPC_H.XPT')\n",
    "hcv_15 = pd.read_sas('laboratory/HEPC_I.XPT')\n",
    "\n",
    "#Hep E\n",
    "hev_09 = pd.read_sas('laboratory/HEPE_F.XPT')\n",
    "hev_11 = pd.read_sas('laboratory/HEPE_G.XPT')\n",
    "hev_13 = pd.read_sas('laboratory/HEPE_H.XPT')\n",
    "hev_15 = pd.read_sas('laboratory/HEPE_I.XPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbsag_99 = hbsag_99.loc[:,['SEQN','LBXHBC','LBDHBG', 'LBDHD']]\n",
    "hbsag_01 = hbsag_01.loc[:,['SEQN','LBXHBC','LBDHBG', 'LBDHD']]\n",
    "hbsag_03 = hbsag_03.loc[:,['SEQN','LBXHBC','LBDHBG', 'LBDHD']]\n",
    "\n",
    "# Concat all for hep\n",
    "hcv_complete_df = pd.concat([hcv_99,hcv_01,hcv_03,hcv_05,hcv_07,\n",
    "                             hcv_09,hcv_11,hcv_13,hcv_15], sort=False)\n",
    "hbsag_complete = pd.concat([hbsag_99, hbsag_01, hbsag_03, hbsag_05, hbsag_07, \n",
    "           hbsag_09, hbsag_11, hbsag_13, hbsag_15])\n",
    "hbsab_complete_df = pd.concat([hbsab_99, hbsab_01, hbsab_03, hbsab_05, hbsab_07, \n",
    "           hbsab_09, hbsab_11, hbsab_13, hbsab_15])\n",
    "hav_complete_df = pd.concat([hav_99, hav_01, hav_03, hav_05, \n",
    "           hav_07, hav_09, hav_11, hav_13, hav_15])\n",
    "hbv_complete_df = hbsag_complete.merge(hbsab_complete_df, on='SEQN', how='outer')\n",
    "hep_complete_df = hbv_complete_df.merge(hav_complete_df, on='SEQN', how='outer').merge(hcv_complete_df, on='SEQN', how='outer')\n",
    "hep_complete_df = hep_complete_df.merge(seqn_year_df,on='SEQN', how='outer')\n",
    "# hep_complete_df.to_csv('HEPATITIS99to16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
